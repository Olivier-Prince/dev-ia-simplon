{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construire un moteur de recherche d'emploi \n",
    "\n",
    "\n",
    "compétences: c1 c2 c3 c4 c5 c20\n",
    "Keywords: Scraping, Scrapy, MongoDB, RSS, Flask, Open Data, client-serveur, URL, Requete HTTP\n",
    "\n",
    "\n",
    "## Description\n",
    "\n",
    "\n",
    "Construire un moteur de recherche d'emploi à partir de données bruts présentes sur le web et develloper une application client-serveur pour acceder aux données.\n",
    "\n",
    "\n",
    "## Contexte\n",
    "\n",
    "\n",
    "Face à la montée du chômage systémique lié aux perturbation sur le marché du travail, les demandeurs d'emploi vont drastiquement augmenter et les emplois evoluer. Cela va occasionner un fort besoin en recherche d'emploi.\n",
    "\n",
    "Dans ce contexte, M. Pontier CEO de la société FlashBot cherche à develloper un moteur de recherche d'emploi et à fait appel à votre expertise. Ils cherchent à construire un système permettant d'agréger des offres d'emploi à partir de données disponible sur l'Internet, à la fois rapidement et de manière innovante. L'objectif étant de faciliter la recherche d'emploi et de maximiser les chances qu'un demandeur d'emploi trouve une offre adaptée pour lui.\n",
    "\n",
    "Plusieurs pateforme existantes offrent des API permettant de rechercher des offres d'emplois. Cependant les données sont dispersés, hétérogènes et ne permettent pas de faire de l'offre adapté.\n",
    "\n",
    "Aujourd'hui M. Pontier cherche à exploiter un **flux RSS** à partir du lien suivant: http://rss.jobsearch.monster.com/rssquery.ashx?q=big data\n",
    "\n",
    "La mission consiste à exploiter ce lien pour récolter le maximum de fiche d'emploi et les stocker en Base De Donnée (BDD) afin de les rendres accessibles et facilliter l'ajout de nouveaux documents.\n",
    "Pour ce faire le client suggère l'utilisation d'une BDD **NoSQL** orienté document dans le but de faciliter:\n",
    "* l'ajout de nouveaux document avec des formats différents\n",
    "* la recherche d'information textuelles\n",
    "\n",
    "Enfin, le client souhaite créer une application client-serveur permettant à un utilisateur de rechercher les offres d'emploi dans sa BDD à partir de requetes **HTTP**.\n",
    "\n",
    "En conclusion, le client vous propose de designer et implémenter le systéme avec les caractéristiques suivantes :\n",
    "* une bot/programme en python pour récolter les données et les envoyer à la BDD. <!--(Attension: Limiter votre nombre de requêtes pour la phase de test pour ne pas vous faire bannir du site cible!)-->\n",
    "* Pour chaque document vous devez créer une entrée dans une BDD orienté document avec tout les champs présent (titre, description, etc)(le client suggére MongoDB) .\n",
    "* Chaque document inséré en BDD devra avoir un champ correspondant au terme de recherche utilisé lors de la requete au flux RSS.\n",
    "* le nombre d'elements dans la bdd doit être optimisé, c'est à dire que les doublons doivent être ecartés.\n",
    "* un application client-serveur doit permettre de faire des requetes à la BDD. Notamment vous devez créer une page permettant de faire une recherche textuelle sur l'ensemble des documents récupérer et de retourner la liste des 10 premiers documents les plus pertinents.\n",
    "\n",
    "### Bonus Tech\n",
    "\n",
    "* améliorer le retour d'une requete client en affichant le nombre de document totale qui ont matché une recherche\n",
    "* Creer une nouvelle route dans votre serveur/backend pour afficher les statistiques de votre base de donnée depuis une requete client: \n",
    "    * nombre de documents totale,\n",
    "    * nombre de mot unique totale dans l'ensemble des documents,\n",
    "    * nombre de mots non unique totale\n",
    "* créer un fichier requirements.txt permettant d'installer automatiquement les dépendances python.\n",
    "\n",
    "### Bonus Data\n",
    "\n",
    "* Comment peut-on mesurer la pertinence pour le classement des documents ? Comment est-elle mesurer ?\n",
    "* afficher la distribution (histogramme) de la répartition des offres d'emploi par métiers.\n",
    "* quelles sont les 20 mots les plus fréquents dans les offres d'emplois ? (En dehors des **stopwords**) le 10 bi-gram les plus fréquents ?\n",
    "* Combien d'offre d'emploi différent avez vous pu trouver sur Monster ? Pouvez vous augmentez ce nombre avec un lexique différent\n",
    "\n",
    "\n",
    "## Ressources\n",
    "\n",
    "\n",
    "mongodb: \n",
    "* https://www.mongodb.com/\n",
    "* https://docs.mongodb.com/\n",
    "scrapy:\n",
    "* https://scrapy.org\n",
    "* https://docs.scrapy.org\n",
    "\n",
    "\n",
    "## Livrable\n",
    "\n",
    "\n",
    "un git par apprenants avec les éléments suivants:\n",
    "1. le bot, qui doit pouvoir être lancé pour alimenter la base de données.\n",
    "2. le serveur, qui doit pouvoir être lancé et servir (ie répondre) à des requetes HTTP.\n",
    "3. un fichier Readme.md qui décrit succinctement votre code et explique comment lancer votre programme, les éventuelles arguments et comment effectuer les requêtes de recherche.\n",
    "\n",
    "\n",
    "## Modalité pédagogique\n",
    "\n",
    "\n",
    "durée: 3 jours  \n",
    "groupe: collaboration en trinôme  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Travail collaboratif sous Kanban Flow\n",
    "\n",
    "https://kanbanflow.com/board/Sr329EA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MongoDB Installation with docker\n",
    "\n",
    "sudo apt install docker.io\n",
    "mkdir -p ~/data-mongodb\n",
    "docker run -d -p 27017:27017 -v ~/src/data/mongo-docker:/data/db --name mongodb mongo:4.2 \n",
    "docker exec -it mongodb mongo\n",
    "\n",
    "# MongoDB server start\n",
    "\n",
    "sudo docker start mongodb\n",
    "\n",
    "# Start the MongoDB client\n",
    "\n",
    "sudo docker exec -it mongodb mongo\n",
    "\n",
    "# Create MonsterDB database in MongoDB\n",
    "\n",
    "> show dbs\n",
    "\n",
    "admin   0.000GB\n",
    "\n",
    "config  0.000GB\n",
    "\n",
    "local   0.000GB\n",
    "\n",
    "> use MonsterDB\n",
    "\n",
    "switched to db MonsterDB\n",
    "\n",
    "> show dbs\n",
    "\n",
    "MonsterDB  0.000GB\n",
    "\n",
    "admin      0.000GB\n",
    "\n",
    "config     0.000GB\n",
    "\n",
    "local      0.000GB\n",
    "\n",
    "# Create ficheJob collection in MonsterDB database\n",
    "\n",
    "> db.createCollection(\"ficheJob\")\n",
    "\n",
    "{ \"ok\" : 1 }\n",
    "\n",
    "# Insert a document in ficheJob collection\n",
    "\n",
    "> db.ficheJob.insert(\n",
    "... {\n",
    "...     title : \"Big Data Developer\",\n",
    "...     description : \"FL-Tampa, Our client, one of the largest financial services firms, is seeking a Big Data Developer Location: Tampa, FL Position Type: Contract Job Responsibilities: -Gather various data points from multiple sources within and load to big data Hadoop solution. -Transform data into usable form per user requirements. -Implement proper controls to protect integrity of data and limit use of sensitive data to una\",\n",
    "... \n",
    "...     link: \"http://jobview.monster.com/Big-Data-Developer-Job-Tampa-FL-US-221978898.aspx\",\n",
    "...     guid: \"221978898\",\n",
    "...     pubDate: \"Thu, 12 Nov 2020 02:31:23 GMT\"\n",
    "...   }\n",
    "...  )\n",
    "\n",
    "WriteResult({ \"nInserted\" : 1 })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'MonsterDB'), 'ficheJob')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pymongo installation & import for python exchange with MongoDB\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient()\n",
    "\n",
    "# Connexion to MonsterDB in MongoDB\n",
    "db = client.MonsterDB\n",
    "collection = db.ficheJob\n",
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('5fb54b0414d931c591f7beb9'),\n",
      " 'description': ['VA-Herndon, Big Data Engineer JobDiva # 20-07366 Find your '\n",
      "                 'next opportunity in Financial Services! Eclaro is looking '\n",
      "                 'for a Big Data Engineer for our client in Herndon, VA. '\n",
      "                 'Eclaro’s client is a leading housing lender, providing '\n",
      "                 'financing for private and commercial for many Americans. If '\n",
      "                 'you’re up to the challenge, then take a chance at this '\n",
      "                 'rewarding opportunity! Responsibilities: Support the team in '\n",
      "                 'the writi'],\n",
      " 'guid': ['222124629'],\n",
      " 'link': ['http://jobview.monster.com/Big-Data-Engineer-Job-Herndon-VA-US-222124629.aspx'],\n",
      " 'pubDate': ['Tue, 17 Nov 2020 14:01:46 GMT'],\n",
      " 'query': 'machine learning',\n",
      " 'title': ['Big Data Engineer']}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "# What are the documents in collection \"ficheJob\" of MonsterDB\n",
    "pprint.pprint(collection.find_one())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['collection', 'ficheJob']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rssquery.ashx'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wget installation & import for downloading RSS query of monster.com\n",
    "import wget\n",
    "\n",
    "url = 'http://rss.jobsearch.monster.com/rssquery.ashx?q=big%20data'\n",
    "flux_RSS = wget.download(url)\n",
    "flux_RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy installation\n",
    "https://docs.scrapy.org/en/latest/intro/tutorial.html\n",
    "\n",
    "# Monster_scrapy project creation in path /home/olivier/Documents/Briefes/20201112_Brief_Moteur_Recherche_Emploi\n",
    "scrapy startproject Monster_scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flashbot integration into Monster_scrapy project / spiders folder\n",
    "\n",
    "-- -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "from scrapy import Request\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "class FlashbotSpider(scrapy.Spider):\n",
    "    name = 'flashbot'\n",
    "    allowed_domains = ['rss.jobsearch.monster.com']\n",
    "\n",
    "    # Start the crawler at this URLs\n",
    "    start_urls = ['file:///home/olivier/Documents/Briefes/20201112_Brief_Moteur_Recherche_Emploi/rssquery.ashx']\n",
    "    # start_urls = ['http://rss.jobsearch.monster.com/rssquery.ashx?q={query}']\n",
    "\n",
    "    #thesaurus = [\"machine learning\", \"machine\", \"learning\", \"big data\", \"big\", \"data\"]\n",
    "    thesaurus = [\"machine learning\"]\n",
    "\n",
    "    LOG_LEVEL = \"INFO\"\n",
    "\n",
    "    def parse(self, response):\n",
    "\n",
    "        # We start with this url\n",
    "        url = self.start_urls[0]\n",
    "\n",
    "        # Build and send a request for each word of the thesaurus\n",
    "        for query in self.thesaurus:\n",
    "            target = url.format(query=query)\n",
    "            print(\"fetching the URL: %s\" % target)\n",
    "            if target.startswith(\"file://\"):\n",
    "                r = Request(target, callback=self.scrapit, dont_filter=True)\n",
    "            else:\n",
    "                r = Request(target, callback=self.scrapit)\n",
    "            r.meta['query'] = query\n",
    "            yield r\n",
    "\n",
    "    def scrapit(self, response):\n",
    "        query = response.meta[\"query\"]\n",
    "\n",
    "        # Scrap the data\n",
    "        for doc in response.xpath(\"//item\"):\n",
    "            # Base item with query used to this response\n",
    "            item = {\"query\": query}\n",
    "\n",
    "            item[\"title\"] = doc.xpath(\"title/text()\").extract()\n",
    "            item[\"description\"] = doc.xpath(\"description/text()\").extract()\n",
    "            item[\"link\"] = doc.xpath(\"link/text()\").extract()\n",
    "            item[\"pubDate\"] = doc.xpath(\"pubDate/text()\").extract()\n",
    "            item[\"guid\"] = doc.xpath(\"guid/text()\").extract()\n",
    "            #pprint(item, indent=2)\n",
    "            print(\"item scraped:\", item[\"title\"])\n",
    "            yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Contraintes M. Pontier\n",
    "\n",
    "les features suivantes dans le bot :\n",
    "\n",
    "* les bot doivent avoir un \"dowload delay\" de 200 millisecondes\n",
    "\n",
    "-- settings.py\n",
    "-- Configure a delay for requests for the same website (default: 0)\n",
    "-- See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
    "-- See also autothrottle settings and docs\n",
    "\n",
    "DOWNLOAD_DELAY = 0.2 # 200 ms of delay\n",
    "\n",
    "* les bots doivent respexcter le robots.txt\n",
    "\n",
    "-- settings.py\n",
    "-- Obey robots.txt rules\n",
    "\n",
    "ROBOTSTXT_OBEY = True\n",
    "\n",
    "* les bots doivents se faire passer pour un Browser/navigateur Opera\n",
    "\n",
    "-- settings.py\n",
    "-- Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "\n",
    "USER_AGENT = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36 OPR/72.0.3815.320'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flashbot execution in path \n",
    "### /home/olivier/Documents/Briefes/20201112_Brief_Moteur_Recherche_Emploi/Monster_scrapy\n",
    "\n",
    "scrapy crawl flashbot -o rss_items.json\n",
    "\n",
    "-- Returned items are stored in rss_items.json file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olivier/anaconda3/envs/dev_IA/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./Monster_scrapy/rss_items.json') as f:\n",
    "    file_data = json.load(f)\n",
    "\n",
    "for ident in file_data:\n",
    "    guid = ident [\"guid\"] [0]\n",
    "    rest = collection.find({\"guid\":guid})\n",
    "    if rest.count() == 0:\n",
    "        collection.insert_one(ident)\n",
    "        print(\"document inserted\", ident [\"title\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.count_documents({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text search in MongoDB shell\n",
    "\n",
    "> db.ficheJob.createIndex( { name: \"text\", description: \"text\" } )\n",
    "\n",
    "{\n",
    "\t\"createdCollectionAutomatically\" : false,\n",
    "\t\"numIndexesBefore\" : 1,\n",
    "\t\"numIndexesAfter\" : 2,\n",
    "\t\"ok\" : 1\n",
    "}\n",
    "\n",
    "> db.ficheJob.find( { $text: { $search: \"hadoop\" } } )\n",
    "\n",
    "{ \"_id\" : ObjectId(\"5fb53d9bcac6d399ed37be2f\"), \"query\" : \"machine learning\", \"title\" : [ \"Big Data Administrator\" ], \"description\" : [ \"DC-Washington, ALTA IT Services is seeking a Sr. Big Data Administrator with at least 7 years of experience. Please contact ebrient@altaits.com Hadoop Administrator Contract ​​​​​​​Washington DC Tasks: • Lead and provide technical solutions and expertise for project teams and customers • Provide daily technical support for the Building environments and Enterprise Data Platforms (EDP) • Analyze, recommend and imp\" ], \"link\" : [ \"http://jobview.monster.com/Big-Data-Administrator-Job-Washington-DC-US-221174788.aspx\" ], \"pubDate\" : [ \"Tue, 17 Nov 2020 08:32:57 GMT\" ], \"guid\" : [ \"221174788\" ] }\n",
    "{ \"_id\" : ObjectId(\"5fb53d9bcac6d399ed37be32\"), \"query\" : \"machine learning\", \"title\" : [ \"Big Data (Hadoop) Developer\" ], \"description\" : [ \"NJ-Newark, Big Data (Hadoop) Developer Location: Newark, NJ Duration: 6+ month contract to hire Interview process: Phone and WebEx Req. # 20-01275 Contact: Brian Anderson D. 952-562-4101 Our client has an opening for a Data Engineer/Data Wrangler who will be responsible for delivering data on-prem and Cloud applications. The ideal candidate would be responsible for developing and delivering Data Lake solutio\" ], \"link\" : [ \"http://jobview.monster.com/Big-Data-Hadoop-Developer-Job-Newark-NJ-US-221298399.aspx\" ], \"pubDate\" : [ \"Tue, 17 Nov 2020 00:05:59 GMT\" ], \"guid\" : [ \"221298399\" ] }\n",
    "{ \"_id\" : ObjectId(\"5fb53d9bcac6d399ed37be38\"), \"query\" : \"machine learning\", \"title\" : [ \"Senior Data Engineer - Remote\" ], \"description\" : [ \"CA-Pasadena, Job Title: Senior Data Engineer - Remote Job Location: REMOTE Job Duration: Direct-hire, Full-Time Requirements: Python, Big Data tools, Hadoop, Spark, AWS, Kubernetes, Docker, ETL Pipelines Must currently have or being willing/able to obtain top secret clearance TS/SCI We are a leading technology and analytics firm, merging the newest tools in AI, Machine Learning, and Big Data to provide new and\" ], \"link\" : [ \"http://jobview.monster.com/Senior-Data-Engineer-Remote-Job-Pasadena-CA-US-222108738.aspx\" ], \"pubDate\" : [ \"Tue, 17 Nov 2020 05:21:13 GMT\" ], \"guid\" : [ \"222108738\" ] }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
