{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construire un moteur de recherche d'emploi  \n",
    "\n",
    "Compétences: c1 c2 c3 c4 c5 c20  \n",
    "Keywords: Scraping, Scrapy, MongoDB, RSS, Flask, Open Data, client-serveur, URL, Requete HTTP  \n",
    "\n",
    "## Description  \n",
    "\n",
    "Construire un moteur de recherche d'emploi à partir de données bruts présentes sur le web et develloper une application client-serveur pour acceder aux données.\n",
    "\n",
    "## Contexte  \n",
    "\n",
    "Face à la montée du chômage systémique liée aux perturbations sur le marché du travail, les demandeurs d'emploi vont drastiquement augmenter et les emplois évoluer.\n",
    "Cela va occasionner un fort besoin en recherche d'emploi.  \n",
    "  \n",
    "Dans ce contexte, M. Pontier CEO de la société FlashBot cherche à développer un moteur de recherche d'emploi et à fait appel à votre expertise.  \n",
    "Il cherche à construire un système permettant d'agréger des offres d'emploi à partir de données disponibles sur l'Internet, à la fois rapidement et de manière innovante.  \n",
    "L'objectif étant de faciliter la recherche d'emploi et de maximiser les chances qu'un demandeur d'emploi trouve une offre adaptée pour lui.  \n",
    "  \n",
    "Plusieurs pateformes existantes offrent des API permettant de rechercher des offres d'emplois. Cependant les données sont dispersées, hétérogènes et ne permettent pas de faire de l'offre adaptée.  \n",
    "\n",
    "Aujourd'hui M. Pontier cherche à exploiter un **flux RSS** à partir du lien suivant : http://rss.jobsearch.monster.com/rssquery.ashx?q=big+data  \n",
    "  \n",
    "La mission consiste à exploiter ce lien pour récolter le maximum de fiches d'emploi et les stocker en BDD afin de les rendre accessibles et faciliter l'ajout de nouveaux documents.\n",
    "Pour ce faire le client suggère l'utilisation d'une BDD **NoSQL** orientée document qui s'appelle MongoDB dans le but de faciliter :  \n",
    "* Ajout de nouveaux documents avec des formats différents  \n",
    "* Recherche d'informations textuelles  \n",
    "  \n",
    "Enfin, le client souhaite créer une application client-serveur permettant à un utilisateur de rechercher les offres d'emploi dans sa BDD à partir de requetes **HTTP**.  \n",
    "  \n",
    "En résumé, le client vous demande de designer et implémenter un systéme avec les caractéristiques suivantes :  \n",
    "* Un bot/programme en python pour récolter les données et les envoyer à la BDD  \n",
    "\n",
    "**Attention ! Limitez votre nombre de requêtes pour la phase de test pour ne pas vous faire bannir du site cible !!!**  \n",
    "\n",
    "* Pour chaque document vous devez créer une entrée dans la BDD orientée document MongoDB avec tout les champs présents (titre, description du poste, etc ...)  \n",
    "* Chaque document inséré en BDD devra avoir un champ correspondant au terme de recherche utilisé lors de la requête au flux RSS  \n",
    "* Le nombre d'éléments dans la BDD doit être optimisé, c'est à dire que les doublons doivent être ecartés  \n",
    "* Une application client-serveur pour permettre de faire des requêtes à la BDD : page permettant de faire une recherche textuelle sur l'ensemble des documents récupérés et de retourner la liste des 10 premiers documents les plus pertinents  \n",
    "\n",
    "### Bonus Tech  \n",
    "\n",
    "* Améliorer le retour d'une requête client en affichant le nombre de documents qui ont matché une recherche  \n",
    "* Créer une nouvelle route dans votre serveur/backend pour afficher les statistiques de votre BDD depuis une requête client :  \n",
    "    * nombre total de documents  \n",
    "    * nombre total de mots uniques dans l'ensemble des documents  \n",
    "    * nombre total de mots non uniques  \n",
    "* Créer un fichier requirements.txt permettant d'installer automatiquement les dépendances python  \n",
    "\n",
    "### Bonus Data  \n",
    "\n",
    "* Comment peut-on mesurer la pertinence pour le classement des documents ? Comment est-elle mesurée ?  \n",
    "* Afficher la distribution (histogramme) de la répartition des offres d'emploi par métier  \n",
    "* Quels sont les 20 mots les plus fréquents dans les offres d'emplois ? (En dehors des **stopwords**) Quels sont les 10 bi-gram les plus fréquents ?  \n",
    "* Combien d'offres d'emploi différentes avez-vous pu trouver sur Monster ? Pouvez-vous augmenter ce nombre avec un lexique différent ?  \n",
    "\n",
    "## Ressources  \n",
    "\n",
    "MongoDB :  \n",
    "* https://www.mongodb.com/  \n",
    "* https://docs.mongodb.com/  \n",
    "\n",
    "Scrapy :  \n",
    "* https://scrapy.org  \n",
    "* https://docs.scrapy.org  \n",
    "\n",
    "## Livrables  \n",
    "\n",
    "Un git par apprenant avec les éléments suivants :  \n",
    "1. Le bot, qui doit pouvoir être lancé pour alimenter la base de données.\n",
    "2. Le serveur, qui doit pouvoir être lancé et servir (ie répondre) à des requetes HTTP.\n",
    "3. Un fichier Readme.md qui décrit succinctement votre code et explique comment lancer votre programme, les éventuels arguments et comment effectuer les requêtes de recherche.\n",
    "\n",
    "## Modalités pédagogiques  \n",
    "\n",
    "* Durée = 3 jours  \n",
    "* Collaboration en trinôme  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travail collaboratif sous Kanban Flow\n",
    "\n",
    "https://kanbanflow.com/board/Sr329EA  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MongoDB Installation with docker\n",
    "\n",
    "```\n",
    "sudo apt install docker.io\n",
    "mkdir -p ~/data-mongodb\n",
    "docker run -d -p 27017:27017 -v ~/src/data/mongo-docker:/data/db --name mongodb mongo:4.2 \n",
    "docker exec -it mongodb mongo\n",
    "```\n",
    "\n",
    "## MongoDB server start\n",
    "\n",
    "`sudo docker start mongodb`\n",
    "\n",
    "## Start the MongoDB client\n",
    "\n",
    "`sudo docker exec -it mongodb mongo`\n",
    "\n",
    "## Create MonsterDB database in MongoDB\n",
    "\n",
    "`> show dbs`\n",
    "\n",
    "admin   0.000GB\n",
    "\n",
    "config  0.000GB\n",
    "\n",
    "local   0.000GB\n",
    "\n",
    "`> use MonsterDB`\n",
    "\n",
    "switched to db MonsterDB\n",
    "\n",
    "`> show dbs`\n",
    "\n",
    "MonsterDB  0.000GB\n",
    "\n",
    "admin      0.000GB\n",
    "\n",
    "config     0.000GB\n",
    "\n",
    "local      0.000GB\n",
    "\n",
    "## Create ficheJob collection in MonsterDB database\n",
    "\n",
    "`> db.createCollection(\"ficheJob\")`\n",
    "\n",
    "{ \"ok\" : 1 }\n",
    "\n",
    "## Insert a document in ficheJob collection\n",
    "\n",
    "```\n",
    "> db.ficheJob.insert(\n",
    "... {\n",
    "...     title : \"Big Data Developer\",\n",
    "...     description : \"FL-Tampa, Our client, one of the largest financial services firms, is seeking a Big Data Developer Location: Tampa, FL Position Type: Contract Job Responsibilities: -Gather various data points from multiple sources within and load to big data Hadoop solution. -Transform data into usable form per user requirements. -Implement proper controls to protect integrity of data and limit use of sensitive data to una\",\n",
    "... \n",
    "...     link: \"http://jobview.monster.com/Big-Data-Developer-Job-Tampa-FL-US-221978898.aspx\",\n",
    "...     guid: \"221978898\",\n",
    "...     pubDate: \"Thu, 12 Nov 2020 02:31:23 GMT\"\n",
    "...   }\n",
    "...  )\n",
    "```\n",
    "\n",
    "WriteResult({ \"nInserted\" : 1 })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'MonsterDB'), 'ficheJob')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pymongo installation & import for python exchange with MongoDB\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient()\n",
    "\n",
    "# Connexion to MonsterDB in MongoDB\n",
    "db = client.MonsterDB\n",
    "collection = db.ficheJob\n",
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('5fb54b0414d931c591f7beb9'),\n",
      " 'description': ['VA-Herndon, Big Data Engineer JobDiva # 20-07366 Find your '\n",
      "                 'next opportunity in Financial Services! Eclaro is looking '\n",
      "                 'for a Big Data Engineer for our client in Herndon, VA. '\n",
      "                 'Eclaro’s client is a leading housing lender, providing '\n",
      "                 'financing for private and commercial for many Americans. If '\n",
      "                 'you’re up to the challenge, then take a chance at this '\n",
      "                 'rewarding opportunity! Responsibilities: Support the team in '\n",
      "                 'the writi'],\n",
      " 'guid': ['222124629'],\n",
      " 'link': ['http://jobview.monster.com/Big-Data-Engineer-Job-Herndon-VA-US-222124629.aspx'],\n",
      " 'pubDate': ['Tue, 17 Nov 2020 14:01:46 GMT'],\n",
      " 'query': 'machine learning',\n",
      " 'title': ['Big Data Engineer']}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "# What are the documents in collection \"ficheJob\" of MonsterDB\n",
    "pprint.pprint(collection.find_one())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['collection', 'ficheJob']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rssquery.ashx'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wget installation & import for downloading RSS query of monster.com\n",
    "import wget\n",
    "\n",
    "url = 'http://rss.jobsearch.monster.com/rssquery.ashx?q=big%20data'\n",
    "flux_RSS = wget.download(url)\n",
    "flux_RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy installation  \n",
    "\n",
    "* https://docs.scrapy.org/en/latest/intro/tutorial.html  \n",
    "\n",
    "## Monster_scrapy project creation  \n",
    "\n",
    "`scrapy startproject Monster_scrapy` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flashbot integration into Monster_scrapy project / spiders folder\n",
    "\n",
    "```\n",
    "-- -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "from scrapy import Request\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "class FlashbotSpider(scrapy.Spider):\n",
    "    name = 'flashbot'\n",
    "    allowed_domains = ['rss.jobsearch.monster.com']\n",
    "\n",
    "    # Start the crawler at this URLs\n",
    "    start_urls = ['file:///home/olivier/Documents/Briefes/20201112_Brief_Moteur_Recherche_Emploi/rssquery.ashx']\n",
    "    # start_urls = ['http://rss.jobsearch.monster.com/rssquery.ashx?q={query}']\n",
    "\n",
    "    #thesaurus = [\"machine learning\", \"machine\", \"learning\", \"big data\", \"big\", \"data\"]\n",
    "    thesaurus = [\"machine learning\"]\n",
    "\n",
    "    LOG_LEVEL = \"INFO\"\n",
    "\n",
    "    def parse(self, response):\n",
    "\n",
    "        # We start with this url\n",
    "        url = self.start_urls[0]\n",
    "\n",
    "        # Build and send a request for each word of the thesaurus\n",
    "        for query in self.thesaurus:\n",
    "            target = url.format(query=query)\n",
    "            print(\"fetching the URL: %s\" % target)\n",
    "            if target.startswith(\"file://\"):\n",
    "                r = Request(target, callback=self.scrapit, dont_filter=True)\n",
    "            else:\n",
    "                r = Request(target, callback=self.scrapit)\n",
    "            r.meta['query'] = query\n",
    "            yield r\n",
    "\n",
    "    def scrapit(self, response):\n",
    "        query = response.meta[\"query\"]\n",
    "\n",
    "        # Scrap the data\n",
    "        for doc in response.xpath(\"//item\"):\n",
    "            # Base item with query used to this response\n",
    "            item = {\"query\": query}\n",
    "\n",
    "            item[\"title\"] = doc.xpath(\"title/text()\").extract()\n",
    "            item[\"description\"] = doc.xpath(\"description/text()\").extract()\n",
    "            item[\"link\"] = doc.xpath(\"link/text()\").extract()\n",
    "            item[\"pubDate\"] = doc.xpath(\"pubDate/text()\").extract()\n",
    "            item[\"guid\"] = doc.xpath(\"guid/text()\").extract()\n",
    "            #pprint(item, indent=2)\n",
    "            print(\"item scraped:\", item[\"title\"])\n",
    "            yield item\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Contraintes M. Pontier  \n",
    "\n",
    "les features suivantes dans le bot :  \n",
    "\n",
    "* les bot doivent avoir un \"dowload delay\" de 200 millisecondes  \n",
    "\n",
    "-- settings.py  \n",
    "-- Configure a delay for requests for the same website (default: 0)  \n",
    "-- See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay  \n",
    "-- See also autothrottle settings and docs  \n",
    "\n",
    "DOWNLOAD_DELAY = 0.2 # 200 ms of delay  \n",
    "\n",
    "* les bots doivent respecter le robots.txt  \n",
    "\n",
    "-- settings.py  \n",
    "-- Obey robots.txt rules  \n",
    "\n",
    "ROBOTSTXT_OBEY = True  \n",
    "\n",
    "* les bots doivents se faire passer pour un Browser/navigateur Opera  \n",
    "\n",
    "-- settings.py  \n",
    "-- Crawl responsibly by identifying yourself (and your website) on the user-agent  \n",
    "\n",
    "USER_AGENT = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36 OPR/72.0.3815.320'  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flashbot execution  \n",
    "\n",
    "`scrapy crawl flashbot -o rss_items.json` \n",
    "\n",
    "-- Returned items are stored in rss_items.json file  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olivier/anaconda3/envs/dev_IA/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./Monster_scrapy/rss_items.json') as f:\n",
    "    file_data = json.load(f)\n",
    "\n",
    "for ident in file_data:\n",
    "    guid = ident [\"guid\"] [0]\n",
    "    rest = collection.find({\"guid\": guid})\n",
    "    if rest.count() == 0:\n",
    "        collection.insert_one(ident)\n",
    "        print(\"document inserted\", ident [\"title\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.count_documents({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text search in MongoDB shell\n",
    "\n",
    "`> db.ficheJob.createIndex( { name: \"text\", description: \"text\" } )`\n",
    "\n",
    "{\n",
    "\t\"createdCollectionAutomatically\" : false,\n",
    "\t\"numIndexesBefore\" : 1,\n",
    "\t\"numIndexesAfter\" : 2,\n",
    "\t\"ok\" : 1\n",
    "}\n",
    "\n",
    "`> db.ficheJob.find( { $text: { $search: \"hadoop\" } } )`\n",
    "\n",
    "{ \"_id\" : ObjectId(\"5fb53d9bcac6d399ed37be2f\"), \"query\" : \"machine learning\", \"title\" : [ \"Big Data Administrator\" ], \"description\" : [ \"DC-Washington, ALTA IT Services is seeking a Sr. Big Data Administrator with at least 7 years of experience. Please contact ebrient@altaits.com Hadoop Administrator Contract ​​​​​​​Washington DC Tasks: • Lead and provide technical solutions and expertise for project teams and customers • Provide daily technical support for the Building environments and Enterprise Data Platforms (EDP) • Analyze, recommend and imp\" ], \"link\" : [ \"http://jobview.monster.com/Big-Data-Administrator-Job-Washington-DC-US-221174788.aspx\" ], \"pubDate\" : [ \"Tue, 17 Nov 2020 08:32:57 GMT\" ], \"guid\" : [ \"221174788\" ] }\n",
    "{ \"_id\" : ObjectId(\"5fb53d9bcac6d399ed37be32\"), \"query\" : \"machine learning\", \"title\" : [ \"Big Data (Hadoop) Developer\" ], \"description\" : [ \"NJ-Newark, Big Data (Hadoop) Developer Location: Newark, NJ Duration: 6+ month contract to hire Interview process: Phone and WebEx Req. # 20-01275 Contact: Brian Anderson D. 952-562-4101 Our client has an opening for a Data Engineer/Data Wrangler who will be responsible for delivering data on-prem and Cloud applications. The ideal candidate would be responsible for developing and delivering Data Lake solutio\" ], \"link\" : [ \"http://jobview.monster.com/Big-Data-Hadoop-Developer-Job-Newark-NJ-US-221298399.aspx\" ], \"pubDate\" : [ \"Tue, 17 Nov 2020 00:05:59 GMT\" ], \"guid\" : [ \"221298399\" ] }\n",
    "{ \"_id\" : ObjectId(\"5fb53d9bcac6d399ed37be38\"), \"query\" : \"machine learning\", \"title\" : [ \"Senior Data Engineer - Remote\" ], \"description\" : [ \"CA-Pasadena, Job Title: Senior Data Engineer - Remote Job Location: REMOTE Job Duration: Direct-hire, Full-Time Requirements: Python, Big Data tools, Hadoop, Spark, AWS, Kubernetes, Docker, ETL Pipelines Must currently have or being willing/able to obtain top secret clearance TS/SCI We are a leading technology and analytics firm, merging the newest tools in AI, Machine Learning, and Big Data to provide new and\" ], \"link\" : [ \"http://jobview.monster.com/Senior-Data-Engineer-Remote-Job-Pasadena-CA-US-222108738.aspx\" ], \"pubDate\" : [ \"Tue, 17 Nov 2020 05:21:13 GMT\" ], \"guid\" : [ \"222108738\" ] }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
